{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation methodology for Chameleon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code has been tested on Python 3.11.7\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from CNN.train import train \n",
    "from inference_pipeline.sliding_window_classification import getModule, classifyTrace, saveClassification\n",
    "from inference_pipeline.segmentation import *\n",
    "from CNN.build_dataset_chameleon import createSubsets\n",
    "from inference_pipeline.debug import *\n",
    "from matplotlib import pyplot as plt\n",
    "from inference_pipeline.heuristic import removeFalseNegatives, removeFalsePositives, removeFalsePositives_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chameleon_path = \"</path/to/chameleon/folder>\"\n",
    "dataset_out_path = \"</path/to/output/folder>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createSubsets(chameleon_path, dataset_out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each CNN is configure thanks to a YALM configuration file.  \n",
    "You can set different module hyper-parameters as well as the dataset, the logger, and the experiment configurations.  \n",
    "Default configuration are in `CNN/configs` directory, both for Neputune logger and for the experiment. \n",
    "\n",
    "> NOTE: some mandatory YALM parameters, such as `dataset_dir`, must be set for each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_folder = \"CNN/configs/chameleon_mrp/\" # /path/to/experiment/config/folder/\n",
    "train(config_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding Window Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SID = \"<Neptune_SID>\"\n",
    "chameleon_file = \"</path/to/trace_file.h5>\"\n",
    "output_file = \"</path/to/output_file.npy>\"\n",
    "\n",
    "module = getModule(SID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and save segmentation as file `output_file`.  \n",
    "Function _classifyTrace_ has a few parameters to configure based on the experiment:\n",
    "\n",
    "- `stride`: Define the stride to use for the sliding window.\n",
    "- `window_size`: Define the size of the sliding window itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 20_000\n",
    "stride = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = classifyTrace(chameleon_file, module, stride, window_size, batch_size=1024)\n",
    "saveClassification(classifications, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the segmenation and find the starting sample of each cryptographic operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_file = \"</path/to/classification_output_file>\"\n",
    "\n",
    "labels = loaderGt(chameleon_file)\n",
    "classifications = np.load(classification_file, mmap_mode='r')\n",
    "\n",
    "gts_starts = []\n",
    "gts_ends = []\n",
    "for label in labels:\n",
    "    gts_starts.append(label['start'])\n",
    "    gts_ends.append(label['end'])\n",
    "\n",
    "init_min_distance = 150_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_filter_size = 150\n",
    "CPs = []\n",
    "\n",
    "for classification, gt, gt_e in tqdm(zip(classifications, gts_starts, gts_ends), total=len(classifications)):\n",
    "    CPs.append(segment(classification, major_filter_size, stride, init_min_distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove false positive and false negatives from the identified COs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts = []\n",
    "ends = []\n",
    "\n",
    "for idx in tqdm(range(len(CPs))):\n",
    "    s, e = removeFalsePositives_basic(CPs[idx]['starts'], CPs[idx]['ends'])\n",
    "    s, e = removeFalseNegatives(s, e, classifications[idx])\n",
    "    starts.append(s)\n",
    "    ends.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnrs_s, fprs_s = [],[]\n",
    "fnrs_e, fprs_e = [],[]\n",
    "\n",
    "for idx in tqdm(range(len(CPs))):\n",
    "    fpr, fnr = errorRate(gts_starts[idx], np.asarray(starts[idx]), stride)\n",
    "    fprs_s.append(fpr)\n",
    "    fnrs_s.append(fnr)\n",
    "    fpr, fnr = errorRate(gts_ends[idx], np.asarray(ends[idx]), stride)\n",
    "    fprs_e.append(fpr)\n",
    "    fnrs_e.append(fnr)\n",
    "    \n",
    "print(f\"Total start mean FPR: {round(np.mean(fprs_s)*100,2)}%\")\n",
    "print(f\"Total start mean FNR: {round(np.mean(fnrs_s)*100,2)}%\")\n",
    "print(f\"Total end mean FPR: {round(np.mean(fprs_e)*100,2)}%\")\n",
    "print(f\"Total end mean FNR: {round(np.mean(fnrs_e)*100,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Visualize the detected COs and ground truth (GT) for a given trace.\n",
    "\n",
    "**First Subplot**: Draws black vertical lines for detected COs' _starts_ and red dashed vertical lines for GT.  \n",
    "**Second Subplot**: Draws grey vertical lines for detected COs' _ends_ and yellow dashed vertical lines for GT.  \n",
    "**Third Subplot**: Plots classification for each point in the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trace = 0\n",
    "center = 100_000\n",
    "margin = 20_000\n",
    "lim = (center-margin, center+margin)\n",
    "\n",
    "fig, ax = plt.subplots(3, figsize=(13, 7))\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "fig.tight_layout(pad=2.0)\n",
    "\n",
    "ax[0].set_xlim(lim)\n",
    "for sample in starts[n_trace][1:]:\n",
    "    ax[0].axvline(x=sample, color='black', linestyle='-')\n",
    "ax[0].axvline(x=starts[n_trace][0], color='black', linestyle='-', label=\"Found CO's start\")\n",
    "\n",
    "for sample in gts_starts[n_trace][1:]:\n",
    "    ax[0].axvline(x=sample//stride, color='r', linestyle='--')\n",
    "ax[0].axvline(x=gts_starts[n_trace][0]//stride, color='r', linestyle='--', label='Start GT')\n",
    "ax[0].legend(loc='lower right', bbox_to_anchor=(1, 0.95))\n",
    "\n",
    "\n",
    "ax[1].set_xlim(lim)\n",
    "for sample in ends[n_trace][1:]:\n",
    "    ax[1].axvline(x=sample, color='grey', linestyle='-')\n",
    "ax[1].axvline(x=ends[n_trace][0], color='grey', linestyle='-', label=\"Found CO's end\")\n",
    "\n",
    "for sample in gts_ends[n_trace][1:]:\n",
    "    ax[1].axvline(x=sample//stride, color='y', linestyle='--')\n",
    "ax[1].axvline(x=gts_ends[n_trace][0]//stride, color='y', linestyle='--', label='End GT')\n",
    "ax[1].legend(loc='lower right', bbox_to_anchor=(1, 0.95))\n",
    "\n",
    "\n",
    "ax[2].set_xlim(lim)\n",
    "ax[2].plot(np.argmax(classifications[n_trace], axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hound",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
